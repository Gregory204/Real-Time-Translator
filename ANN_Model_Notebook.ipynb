{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FqiMOBOpwzG"
      },
      "source": [
        "# 1. Data Preparation and Feature Extraction\n",
        "\n",
        "- First, install & load dependencies.\n",
        "\n",
        "- Ensure you have a dataset with audio samples for each speaker.\n",
        "\n",
        "- Define a feature extraction function that - extracts relevant audio features for each speaker. In this case, the extract_feature function uses Mel spectrograms to create a feature vector, which is suitable for speaker identification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuR_E5bvuYtn"
      },
      "source": [
        "## 1.1 Install Our Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUSDpBa9R4Yb"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow tensorflow-io matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aid-ugcbuXnN"
      },
      "source": [
        "## 1.2 Load Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h6doqjV8ugB9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import librosa\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io.wavfile import write, read as wav_read"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLLrDXLiv1ne"
      },
      "source": [
        "## 1.3 Lets Get Our Data In!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "M_7z6RoiwEbq"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # This will prompt you to upload the kaggle.json file\n",
        "\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/\"\n",
        "\n",
        "!kaggle datasets download -d mfekadu/english-multispeaker-corpus-for-voice-cloning/\n",
        "\n",
        "!unzip -qq english-multispeaker-corpus-for-voice-cloning.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Feature Extraction Function"
      ],
      "metadata": {
        "id": "cnhel9_g0bzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract features from audio file\n",
        "def extract_feature(file_name):\n",
        "    \"\"\" Extract features from audio file\n",
        "    Args:\n",
        "      file_name (str): Path to audio file\n",
        "\n",
        "    return:\n",
        "      np.array: Feature vector\n",
        "    \"\"\"\n",
        "    X, sample_rate = librosa.core.load(file_name) # load audio file\n",
        "    result = np.array([]) # array that stores features\n",
        "    mel = np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0) # calc mel spectogram\n",
        "    result = np.hstack((result, mel)) # insert the mel spect into results arr\n",
        "    return result # return the feature vector"
      ],
      "metadata": {
        "id": "r_cP-uKf0hFu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.45 Create Var for Data Root!"
      ],
      "metadata": {
        "id": "3L-UYo1s-HbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable that holds path to wav files (txts r washed)\n",
        "DATA_ROOT = '/content/VCTK-Corpus/VCTK-Corpus/wav48'"
      ],
      "metadata": {
        "id": "KNTlxx7H-Ok_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Speaker Subset Selection and Preprocessing\n",
        "\n",
        "- Define a subset of speakers you want to use for training and testing\n",
        "\n",
        "- Create a function that will process audio files for the subset of speakers"
      ],
      "metadata": {
        "id": "I1VrBden9Fy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target speakers in a list\n",
        "target_speakers = [\n",
        "    \"p225\", # lone tone female\n",
        "    \"p228\", # medium tone female\n",
        "    \"p236\", # high tone female\n",
        "    \"p249\", # low tone female\n",
        "    \"p257\", # medium tone female\n",
        "    \"p226\", # medium tone male\n",
        "    \"p237\", # low tone male\n",
        "    \"p241\", # medium tone male\n",
        "    \"p304\", # low tone male\n",
        "    \"p326\"  # low tone male\n",
        "]\n",
        "\n",
        "# FUN FACT O(N^2) runtime right? --> ?_?\n",
        "\n",
        "# Function to process audio files for target speakers\n",
        "def process_audio_files(data_directory):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    for speaker_id in target_speakers:\n",
        "        speaker_dir = os.path.join(data_directory, speaker_id)\n",
        "        wav_files = sorted([f for f in os.listdir(speaker_dir) if f.endswith(\".wav\")])\n",
        "\n",
        "        # Exclude the last 5 files for unseen data!!!!\n",
        "        for file_name in wav_files[:-5]:\n",
        "            file_path = os.path.join(speaker_dir, file_name)\n",
        "            feature = extract_feature(file_path)\n",
        "            features.append(feature)\n",
        "            labels.append(speaker_id)\n",
        "        print(f'Last 5 for {speaker_id}: {wav_files[-5:]}')\n",
        "    return features, labels\n",
        "\n",
        "# Grab features and labels from DATA_ROOT\n",
        "features, labels = process_audio_files(DATA_ROOT)"
      ],
      "metadata": {
        "id": "bIJEwboe7pI-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18db5941-f11d-47ba-d52e-3e3c598083d4"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last 5 for p225: ['p225_358.wav', 'p225_359.wav', 'p225_363.wav', 'p225_365.wav', 'p225_366.wav']\n",
            "Last 5 for p226: ['p226_366.wav', 'p226_367.wav', 'p226_368.wav', 'p226_369.wav', 'p226_370.wav']\n",
            "Last 5 for p228: ['p228_367.wav', 'p228_368.wav', 'p228_369.wav', 'p228_370.wav', 'p228_371.wav']\n",
            "Last 5 for p236: ['p236_499.wav', 'p236_500.wav', 'p236_501.wav', 'p236_502.wav', 'p236_503.wav']\n",
            "Last 5 for p237: ['p237_347.wav', 'p237_348.wav', 'p237_349.wav', 'p237_350.wav', 'p237_351.wav']\n",
            "Last 5 for p241: ['p241_370.wav', 'p241_371.wav', 'p241_372.wav', 'p241_373.wav', 'p241_374.wav']\n",
            "Last 5 for p249: ['p249_350.wav', 'p249_351.wav', 'p249_352.wav', 'p249_353.wav', 'p249_354.wav']\n",
            "Last 5 for p257: ['p257_430.wav', 'p257_431.wav', 'p257_432.wav', 'p257_433.wav', 'p257_434.wav']\n",
            "Last 5 for p304: ['p304_420.wav', 'p304_421.wav', 'p304_422.wav', 'p304_423.wav', 'p304_424.wav']\n",
            "Last 5 for p326: ['p326_396.wav', 'p326_397.wav', 'p326_398.wav', 'p326_399.wav', 'p326_400.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Lets see an example on what features and labels looks like? :D"
      ],
      "metadata": {
        "id": "pl3Z0D1D79Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(features[0])  # Print the first feature vector\n",
        "print(labels[0])    # Print the corresponding label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p8SHVA7W_0IG",
        "outputId": "a9d330e2-719a-425b-e2f9-63a729e7b3a9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- All that data just for only one wav file btw. But remember.\n",
        "\n",
        "- Giant Array = Features\n",
        "- String \"p225\" = Label"
      ],
      "metadata": {
        "id": "30VJDwlQ_3u3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewm2b1eyvE9i"
      },
      "source": [
        "# 3. Build Model And Train Model\n",
        "\n",
        "- Instead of using a binary classification model, the speaker classification will be a multiclass model where each class represents a unique speaker.\n",
        "\n",
        "- The models architecture has to make sure the output layer has neurons equal to number of speaker in the subset. So 10. Also softmax activation from multiclass classification."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Model Architecture"
      ],
      "metadata": {
        "id": "jxHYn6RcFooH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model for both Genders if predictions are great for final product!\n",
        "\n",
        "def create_speaker_model(vector_length=128, num_speakers=9):\n",
        "    model = Sequential([\n",
        "        Dense(256, input_shape=(vector_length,), activation='relu'), # 256 neurons, Relu Activation\n",
        "\n",
        "        Dropout(0.3), # randomly turn off 30% of neruons to prevent overfitting\n",
        "\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'), # 128 neurons\n",
        "        Dropout(0.3),\n",
        "        Dense(num_speakers, activation='softmax')  # For multiclass classification ('softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "XHUpJiMMaSUH"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Categorical Crossentropy: Measures the difference between predicted and actual class probabilities for multiclass tasks.\n",
        "\n",
        "- Adam Optimizer: Adjusts learning rates during training to optimize model accuracy faster.\n",
        "\n",
        "- Neurons: Basic units that represent learned patterns in the data.\n",
        "\n",
        "- ReLU: Activation that converts negative values to zero."
      ],
      "metadata": {
        "id": "x5HdnkGmGARH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Example of Neuron (From GPT):\n",
        "\n",
        "\"\"\"\n",
        "A neuron in a neural network is a computational unit that takes in one or more inputs, applies weights, adds a bias, and then passes the result through an activation function to produce an output.\n",
        "\n",
        "Imagine a neuron designed to predict whether an email is spam or not based on just two features: the number of suspicious keywords and the presence of a link.\n",
        "\"\"\"\n",
        "\n",
        "- in our case the activation function is ReLu"
      ],
      "metadata": {
        "id": "crxCe7dWGXyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Train Test Split & One Hot Encoding"
      ],
      "metadata": {
        "id": "fGHZ0apsHd-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Assuming `X_train` is your feature matrix and `y_train` is your one-hot encoded label matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np_features = np.array(features)\n",
        "np_labels = np.array(labels)\n",
        "\n",
        "# Split your data into training and validation sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(np_features, np_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "encoder = OneHotEncoder(sparse_output=False,\n",
        "                        handle_unknown='ignore') # sparse=False for dense output\n",
        "\n",
        "encoder.fit(y_train.reshape(-1, 1)) # Fit on training labels, reshaped for 2D input\n",
        "\n",
        "y_train_encoded = encoder.transform(y_train.reshape(-1, 1))\n",
        "y_test_encoded = encoder.transform(y_test.reshape(-1, 1))\n",
        "\n",
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"y_train shape:\", y_train_encoded.shape)\n",
        "print(\"X_val shape:\", X_test.shape)\n",
        "print(\"y_val shape:\", y_test_encoded.shape)"
      ],
      "metadata": {
        "id": "77zBw-V6bDzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e222f5c2-28a1-489b-8581-fb6639aaecee"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (2942, 128)\n",
            "y_train shape: (2942, 10)\n",
            "X_val shape: (736, 128)\n",
            "y_val shape: (736, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Before One Hot encoding the model would treat the speaker IDs just as numerical values which leads to incorrect predictions.\n",
        "\n",
        "- After One Hot Encoding th model now can learn the relationship between audio features and speaker identities! :D"
      ],
      "metadata": {
        "id": "jt5Zwfj_KEDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Initialize and View Summary of Model"
      ],
      "metadata": {
        "id": "QHTGHLGCFnBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "speaker_model = create_speaker_model(vector_length=X_train.shape[1], num_speakers=y_train_encoded.shape[1])\n",
        "\n",
        "print(speaker_model.summary())"
      ],
      "metadata": {
        "id": "tXEdyH6Xa58P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "a12f4e8b-860f-4638-b2bf-031657f277bb"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m33,024\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m65,792\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │           \u001b[38;5;34m1,290\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m133,002\u001b[0m (519.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">133,002</span> (519.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m133,002\u001b[0m (519.54 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">133,002</span> (519.54 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.4 Train The MLP (Multi-Layer Perceptron) Model"
      ],
      "metadata": {
        "id": "bmd7rvMPKzri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "speaker_model.fit(X_train, y_train_encoded, validation_data=(X_test, y_test_encoded), epochs=30, batch_size=32)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oL-ojjPAKqsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # 4. Evaluate The Model"
      ],
      "metadata": {
        "id": "ZTkiOLvRLPNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Make Predictions"
      ],
      "metadata": {
        "id": "eRqxbzauLXkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probs = speaker_model.predict(X_test)  # Get predicted probabilities\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)      # Convert to class predictions\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrqhJoCqLS56",
        "outputId": "5e667279-3d17-4211-c64a-3fdc8fc6152b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Import And Calculate Metrics"
      ],
      "metadata": {
        "id": "NyYm9aVELaJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "\n",
        "accuracy = accuracy_score(np.argmax(y_test_encoded, axis=1), y_pred)\n",
        "precision = precision_score(np.argmax(y_test_encoded, axis=1), y_pred, average='weighted')  # For multiclass\n",
        "recall = recall_score(np.argmax(y_test_encoded, axis=1), y_pred, average='weighted')      # For multiclass\n",
        "f1 = f1_score(np.argmax(y_test_encoded, axis=1), y_pred, average='weighted')              # For multiclass"
      ],
      "metadata": {
        "id": "kmr3mCh-LgRw"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Accuracy: Measure how often the model correctly predicts the speaker.\n",
        "\n",
        "- Precision: Check how many of the speakers predicted as a certain class truly belong to that class.\n",
        "\n",
        "- Recall: Examine how many of the actual instances of a speaker are correctly identified by the model.\n",
        "\n",
        "- F1-Score: Consider the balance between precision and recall, particularly valuable if there is an imbalance in speaker samples."
      ],
      "metadata": {
        "id": "qeEwXXA9n71C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")\n",
        "print(f\"F1-score: {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtnn4qKTLqP9",
        "outputId": "4a279ca0-88fa-483c-f8da-ef82063242db"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9538043478260869\n",
            "Precision: 0.9543011808554899\n",
            "Recall: 0.9538043478260869\n",
            "F1-score: 0.9533499422908044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Results Looking a bit too good to be true... >:/"
      ],
      "metadata": {
        "id": "I9UVWS0ZM29f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Classification Report And Confusion Matrix"
      ],
      "metadata": {
        "id": "BqUkT4WgLx0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(np.argmax(y_test_encoded, axis=1), y_pred, target_names=encoder.categories_[0]))  # Report with speaker IDs\n",
        "cm = confusion_matrix(np.argmax(y_test_encoded, axis=1), y_pred)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZo2szq2L9Au",
        "outputId": "8f0f7894-85d8-48ff-c1f7-533c9b9b76b2"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        p225       1.00      0.89      0.94        55\n",
            "        p226       0.95      0.95      0.95        88\n",
            "        p228       0.92      0.82      0.86        66\n",
            "        p236       0.96      1.00      0.98        94\n",
            "        p237       1.00      0.98      0.99        65\n",
            "        p241       0.96      0.97      0.96        68\n",
            "        p249       0.93      1.00      0.96        62\n",
            "        p257       0.88      0.91      0.89        79\n",
            "        p304       0.98      0.98      0.98        84\n",
            "        p326       0.99      1.00      0.99        75\n",
            "\n",
            "    accuracy                           0.95       736\n",
            "   macro avg       0.96      0.95      0.95       736\n",
            "weighted avg       0.95      0.95      0.95       736\n",
            "\n",
            "Confusion Matrix:\n",
            "[[49  0  1  1  0  0  2  1  0  1]\n",
            " [ 0 84  0  0  0  3  0  0  1  0]\n",
            " [ 0  0 54  0  0  0  3  9  0  0]\n",
            " [ 0  0  0 94  0  0  0  0  0  0]\n",
            " [ 0  1  0  0 64  0  0  0  0  0]\n",
            " [ 0  2  0  0  0 66  0  0  0  0]\n",
            " [ 0  0  0  0  0  0 62  0  0  0]\n",
            " [ 0  0  4  2  0  0  0 72  1  0]\n",
            " [ 0  1  0  1  0  0  0  0 82  0]\n",
            " [ 0  0  0  0  0  0  0  0  0 75]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "hmmmmmmm...... lets test it >:o"
      ],
      "metadata": {
        "id": "OUKdrVhZM84p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 TEST IT URSELF!"
      ],
      "metadata": {
        "id": "sNpR-HidMNXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Used GPT to generate this amazing test function for me."
      ],
      "metadata": {
        "id": "r_cZ6Y7HXIJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def test_speaker_model(model, encoder, data_root, test_cases):\n",
        "  \"\"\"\n",
        "  Tests the speaker model using provided test cases.\n",
        "\n",
        "  Args:\n",
        "    model: The trained speaker model.\n",
        "    encoder: The OneHotEncoder used for label encoding.\n",
        "    data_root: The path to the dataset root directory.\n",
        "    test_cases: A dictionary mapping speaker IDs to file names or lists of file names.\n",
        "\n",
        "  Returns:\n",
        "    float: The accuracy of the model on the test cases.\n",
        "  \"\"\"\n",
        "  actual_labels = []\n",
        "  predicted_labels = []\n",
        "\n",
        "  for speaker_id, file_names in test_cases.items():\n",
        "    # Handle single file name or list of file names\n",
        "    if isinstance(file_names, str):\n",
        "        file_names = [file_names]  # Convert single file name to a list\n",
        "\n",
        "    for file_name in file_names:\n",
        "        # get our filepath for user\n",
        "        file_path = os.path.join(data_root, speaker_id, file_name + \".wav\")\n",
        "\n",
        "        # grab its features\n",
        "        feature = extract_feature(file_path)\n",
        "\n",
        "        # reshape so it matches when testing\n",
        "        feature = feature.reshape(1, -1)\n",
        "\n",
        "        prediction_probs = model.predict(feature) # grab prob of speaker\n",
        "        predicted_speaker_index = np.argmax(prediction_probs) # get speaker with highest prob\n",
        "        predicted_speaker_id = encoder.categories_[0][predicted_speaker_index]\n",
        "\n",
        "        actual_labels.append(speaker_id) # append real\n",
        "        predicted_labels.append(predicted_speaker_id) # append predicted\n",
        "\n",
        "  accuracy = accuracy_score(actual_labels, predicted_labels) # how accurate were we?\n",
        "  return accuracy\n",
        "\n",
        "# Define your test cases\n",
        "test_cases = {\n",
        "    \"p225\": [\"p225_358\", \"p225_359\", \"p225_363\", \"p225_365\", \"p225_366\"],\n",
        "    \"p226\": [\"p226_366\", \"p226_367\", \"p226_368\", \"p226_369\", \"p226_370\"],\n",
        "    \"p228\": [\"p228_367\", \"p228_368\", \"p228_369\", \"p228_370\", \"p228_371\"],\n",
        "    \"p236\": [\"p236_499\", \"p236_500\", \"p236_501\", \"p236_502\", \"p236_503\"],\n",
        "    \"p237\": [\"p237_347\", \"p237_348\", \"p237_349\", \"p237_350\", \"p237_351\"],\n",
        "    \"p241\": [\"p241_370\", \"p241_371\", \"p241_372\", \"p241_373\", \"p241_374\"],\n",
        "    \"p249\": [\"p249_350\", \"p249_351\", \"p249_352\", \"p249_353\", \"p249_354\"],\n",
        "    \"p257\": [\"p257_430\", \"p257_431\", \"p257_432\", \"p257_433\", \"p257_434\"],\n",
        "    \"p304\": [\"p304_420\", \"p304_421\", \"p304_422\", \"p304_423\", \"p304_424\"],\n",
        "    \"p326\": [\"p326_396\", \"p326_397\", \"p326_398\", \"p326_399\", \"p326_400\"]\n",
        "}\n",
        "\n",
        "# Run the test and print the accuracy\n",
        "accuracy = test_speaker_model(speaker_model, encoder, DATA_ROOT, test_cases)  # Using your trained model and encoder\n",
        "print(f\"Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4lXLJlcVgweV",
        "outputId": "58386cf7-4349-416f-f7df-350056e2db65"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Test Accuracy: 0.94\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- I am so shocked at how well this model is doing compared to before. It was worth investing time into this!"
      ],
      "metadata": {
        "id": "MrEMFGY_XUut"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Save Model! :D\n",
        "\n",
        "- Now we can use the model in streamlit, gradio, or wherever."
      ],
      "metadata": {
        "id": "cgF0lyzhVGg1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "speaker_model.save('CUR_speaker_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S73n8HLFnW9I",
        "outputId": "eb3cbabd-b60c-48b3-c909-1d7042f45388"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CONTINUING MALE FEMALE MODEL\n",
        "\n",
        "- this most likely will not be used since the model I am currently using is doing really good but just in case ill add it here..."
      ],
      "metadata": {
        "id": "sEFa_7VWTRo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Male Model\n",
        "Male_speakers = [\n",
        "    \"p226\", # medium tone male\n",
        "    \"p237\", # low tone male\n",
        "    \"p241\", # medium tone male\n",
        "    \"p304\", # low tone male\n",
        "    \"p326\"  # low tone male\n",
        "]\n",
        "\n",
        "# Function to process audio files for target female speakers\n",
        "def m_process_audio_files(data_directory):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # lets iterate shall we? :D\n",
        "    for speaker_id in Male_speakers:\n",
        "\n",
        "        # get path associated to speaker_id\n",
        "        speaker_dir = os.path.join(data_directory, speaker_id)\n",
        "\n",
        "        # iterate again? >:D  (btw we itertating thru individual folders)\n",
        "        for file_name in os.listdir(speaker_dir):\n",
        "\n",
        "            # just wanna make sure we get only WAV\n",
        "            if file_name.endswith(\".wav\"):\n",
        "\n",
        "                # get file path associated to speaker\n",
        "                file_path = os.path.join(speaker_dir, file_name)\n",
        "\n",
        "                # extract features from speakers file_path (of speaker)\n",
        "                feature = extract_feature(file_path)\n",
        "\n",
        "                # append the features to list\n",
        "                features.append(feature)\n",
        "\n",
        "                # append lable to list (ex: p225)\n",
        "                labels.append(speaker_id)\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# Grab features and labels from DATA_ROOT\n",
        "m_features, m_labels = m_process_audio_files(DATA_ROOT)"
      ],
      "metadata": {
        "id": "rat5AlyTPQgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For Female Model\n",
        "\n",
        "Female_speakers = [\n",
        "    \"p225\", # lone tone female\n",
        "    \"p228\", # medium tone female\n",
        "    \"p236\", # high tone female\n",
        "    \"p249\", # low tone female\n",
        "    \"p257\", # medium tone female\n",
        "]\n",
        "\n",
        "# Function to process audio files for target female speakers\n",
        "def f_process_audio_files(data_directory):\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # lets iterate shall we? :D\n",
        "    for speaker_id in Female_speakers:\n",
        "\n",
        "        # get path associated to speaker_id\n",
        "        speaker_dir = os.path.join(data_directory, speaker_id)\n",
        "\n",
        "        # iterate again? >:D  (btw we itertating thru individual folders)\n",
        "        for file_name in os.listdir(speaker_dir):\n",
        "\n",
        "            # just wanna make sure we get only WAV\n",
        "            if file_name.endswith(\".wav\"):\n",
        "\n",
        "                # get file path associated to speaker\n",
        "                file_path = os.path.join(speaker_dir, file_name)\n",
        "\n",
        "                # extract features from speakers file_path (of speaker)\n",
        "                feature = extract_feature(file_path)\n",
        "\n",
        "                # append the features to list\n",
        "                features.append(feature)\n",
        "\n",
        "                # append lable to list (ex: p225)\n",
        "                labels.append(speaker_id)\n",
        "\n",
        "    return features, labels\n",
        "\n",
        "# Grab features and labels from DATA_ROOT\n",
        "f_features, f_labels = f_process_audio_files(DATA_ROOT)"
      ],
      "metadata": {
        "id": "QAjOqMDzOvU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For the females"
      ],
      "metadata": {
        "id": "s1W1WhWyPVJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(f_features[0])  # Print the first feature vector\n",
        "print(f_labels[0])    # Print the corresponding label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbC9zwsQPUng",
        "outputId": "50aacb22-b58d-4b57-84f8-3c63fbf5cf76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- For the males"
      ],
      "metadata": {
        "id": "ChH-tZR0P96Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(m_features[0])  # Print the first feature vector\n",
        "print(m_labels[0])    # Print the corresponding label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "m90JifT8QA1M",
        "outputId": "0906f4f1-3c4e-4ae9-fc43-8bca15e58f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def M_F_create_speaker_model(vector_length=128, num_speakers=5):\n",
        "    model = Sequential([\n",
        "        Dense(256, input_shape=(vector_length,), activation='relu'), # 256 neurons, Relu Activation\n",
        "\n",
        "        Dropout(0.3), # randomly turn off 30% of neruons to prevent overfitting\n",
        "\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(128, activation='relu'), # 128 neurons\n",
        "        Dropout(0.3),\n",
        "        Dense(num_speakers, activation='softmax')  # For multiclass classification ('softmax')\n",
        "    ])\n",
        "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
        "    return model"
      ],
      "metadata": {
        "id": "g-JvEoPIQmow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Assuming `X_train` is your feature matrix and `y_train` is your one-hot encoded label matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "f_np_features = np.array(f_features)\n",
        "f_np_labels = np.array(f_labels)\n",
        "\n",
        "# Split your data into training and validation sets\n",
        "f_X_train, f_X_test, f_y_train, f_y_test = train_test_split(f_np_features, f_np_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "f_encoder = OneHotEncoder(sparse_output=False,\n",
        "                        handle_unknown='ignore') # sparse=False for dense output\n",
        "\n",
        "f_encoder.fit(f_y_train.reshape(-1, 1)) # Fit on training labels, reshaped for 2D input\n",
        "\n",
        "f_y_train_encoded = f_encoder.transform(f_y_train.reshape(-1, 1))\n",
        "f_y_test_encoded = f_encoder.transform(f_y_test.reshape(-1, 1))\n",
        "\n",
        "print(\"Female X_train shape:\", f_X_train.shape)\n",
        "print(\"Female y_train shape:\", f_y_train_encoded.shape)\n",
        "print(\"Female X_val shape:\", f_X_test.shape)\n",
        "print(\"Female y_val shape:\", f_y_test_encoded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuskDtWbQx8T",
        "outputId": "d2d3a72b-058e-4c35-8d33-17252a4cf3e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Female X_train shape: (1486, 128)\n",
            "Female y_train shape: (1486, 5)\n",
            "Female X_val shape: (372, 128)\n",
            "Female y_val shape: (372, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "female_speaker_model = M_F_create_speaker_model(vector_length=f_X_train.shape[1], num_speakers=f_y_train_encoded.shape[1])\n",
        "\n",
        "print(female_speaker_model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "tsg0MmhaT2Le",
        "outputId": "8ee708dd-04b2-4823-f056-a3f47c539c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m33,024\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m65,792\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m645\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m132,357\u001b[0m (517.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">132,357</span> (517.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m132,357\u001b[0m (517.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">132,357</span> (517.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the female model\n",
        "female_speaker_model.fit(f_X_train, f_y_train_encoded, validation_data=(f_X_test, f_y_test_encoded), epochs=30, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GDz4mOq9UOVD",
        "outputId": "b63fff6b-91b0-4aec-cc2b-9880cafe1624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 53ms/step - accuracy: 0.4504 - loss: 1.5939 - val_accuracy: 0.7796 - val_loss: 0.5938\n",
            "Epoch 2/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7447 - loss: 0.6864 - val_accuracy: 0.8575 - val_loss: 0.4203\n",
            "Epoch 3/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8022 - loss: 0.5287 - val_accuracy: 0.8656 - val_loss: 0.3639\n",
            "Epoch 4/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8131 - loss: 0.4900 - val_accuracy: 0.8925 - val_loss: 0.3024\n",
            "Epoch 5/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8290 - loss: 0.4025 - val_accuracy: 0.9005 - val_loss: 0.2718\n",
            "Epoch 6/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8560 - loss: 0.3919 - val_accuracy: 0.9140 - val_loss: 0.2207\n",
            "Epoch 7/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8826 - loss: 0.3497 - val_accuracy: 0.9140 - val_loss: 0.2313\n",
            "Epoch 8/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8989 - loss: 0.2601 - val_accuracy: 0.9220 - val_loss: 0.2219\n",
            "Epoch 9/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9142 - loss: 0.2528 - val_accuracy: 0.9274 - val_loss: 0.2387\n",
            "Epoch 10/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9109 - loss: 0.2495 - val_accuracy: 0.9167 - val_loss: 0.2233\n",
            "Epoch 11/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8969 - loss: 0.2715 - val_accuracy: 0.9328 - val_loss: 0.1862\n",
            "Epoch 12/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9361 - loss: 0.1821 - val_accuracy: 0.9274 - val_loss: 0.1778\n",
            "Epoch 13/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9283 - loss: 0.2230 - val_accuracy: 0.9355 - val_loss: 0.2158\n",
            "Epoch 14/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9485 - loss: 0.1725 - val_accuracy: 0.9328 - val_loss: 0.1909\n",
            "Epoch 15/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9341 - loss: 0.1859 - val_accuracy: 0.9328 - val_loss: 0.1709\n",
            "Epoch 16/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9369 - loss: 0.1751 - val_accuracy: 0.9247 - val_loss: 0.1907\n",
            "Epoch 17/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9422 - loss: 0.1540 - val_accuracy: 0.9355 - val_loss: 0.1545\n",
            "Epoch 18/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9501 - loss: 0.1263 - val_accuracy: 0.9435 - val_loss: 0.1674\n",
            "Epoch 19/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9367 - loss: 0.1554 - val_accuracy: 0.9382 - val_loss: 0.1628\n",
            "Epoch 20/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9666 - loss: 0.0968 - val_accuracy: 0.9328 - val_loss: 0.1791\n",
            "Epoch 21/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9478 - loss: 0.1432 - val_accuracy: 0.9462 - val_loss: 0.1695\n",
            "Epoch 22/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9551 - loss: 0.1291 - val_accuracy: 0.9301 - val_loss: 0.2181\n",
            "Epoch 23/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9614 - loss: 0.1374 - val_accuracy: 0.9597 - val_loss: 0.1406\n",
            "Epoch 24/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9549 - loss: 0.1151 - val_accuracy: 0.9516 - val_loss: 0.1354\n",
            "Epoch 25/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9597 - loss: 0.1134 - val_accuracy: 0.9677 - val_loss: 0.1071\n",
            "Epoch 26/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9587 - loss: 0.1213 - val_accuracy: 0.9570 - val_loss: 0.1208\n",
            "Epoch 27/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9728 - loss: 0.0876 - val_accuracy: 0.9570 - val_loss: 0.1405\n",
            "Epoch 28/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.1011 - val_accuracy: 0.9489 - val_loss: 0.1750\n",
            "Epoch 29/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9602 - loss: 0.1419 - val_accuracy: 0.9409 - val_loss: 0.1326\n",
            "Epoch 30/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9633 - loss: 0.1059 - val_accuracy: 0.9516 - val_loss: 0.1250\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c0bd894d7b0>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Assuming `X_train` is your feature matrix and `y_train` is your one-hot encoded label matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "m_np_features = np.array(m_features)\n",
        "m_np_labels = np.array(m_labels)\n",
        "\n",
        "# Split your data into training and validation sets\n",
        "m_X_train, m_X_test, m_y_train, m_y_test = train_test_split(m_np_features, m_np_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "m_encoder = OneHotEncoder(sparse_output=False,\n",
        "                        handle_unknown='ignore') # sparse=False for dense output\n",
        "\n",
        "m_encoder.fit(m_y_train.reshape(-1, 1)) # Fit on training labels, reshaped for 2D input\n",
        "\n",
        "m_y_train_encoded = m_encoder.transform(m_y_train.reshape(-1, 1))\n",
        "m_y_test_encoded = m_encoder.transform(m_y_test.reshape(-1, 1))\n",
        "\n",
        "print(\"Male X_train shape:\", m_X_train.shape)\n",
        "print(\"Male y_train shape:\", m_y_train_encoded.shape)\n",
        "print(\"Male X_val shape:\", m_X_test.shape)\n",
        "print(\"Male y_val shape:\", m_y_test_encoded.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8Qkl6MRTPrS",
        "outputId": "507a5193-b577-481d-d687-80478e531e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Male X_train shape: (1496, 128)\n",
            "Male y_train shape: (1496, 5)\n",
            "Male X_val shape: (374, 128)\n",
            "Male y_val shape: (374, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "male_speaker_model = M_F_create_speaker_model(vector_length=m_X_train.shape[1], num_speakers=m_y_train_encoded.shape[1])\n",
        "\n",
        "print(male_speaker_model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "SYnm6cwgUGXq",
        "outputId": "03b6b1ec-8578-45ea-9a89-ec4b325294de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m33,024\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m65,792\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │             \u001b[38;5;34m645\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">645</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m132,357\u001b[0m (517.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">132,357</span> (517.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m132,357\u001b[0m (517.02 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">132,357</span> (517.02 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Male model\n",
        "male_speaker_model.fit(m_X_train, m_y_train_encoded, validation_data=(m_X_test, m_y_test_encoded), epochs=30, batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "smZaZqm3UrFS",
        "outputId": "e81ce9c4-e181-4ff0-b5f4-1ea4e76b184d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 111ms/step - accuracy: 0.4677 - loss: 1.3097 - val_accuracy: 0.9144 - val_loss: 0.4009\n",
            "Epoch 2/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9149 - loss: 0.4052 - val_accuracy: 0.9599 - val_loss: 0.1418\n",
            "Epoch 3/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9415 - loss: 0.2036 - val_accuracy: 0.9572 - val_loss: 0.1065\n",
            "Epoch 4/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9492 - loss: 0.1530 - val_accuracy: 0.9572 - val_loss: 0.1066\n",
            "Epoch 5/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9624 - loss: 0.1135 - val_accuracy: 0.9706 - val_loss: 0.0834\n",
            "Epoch 6/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9626 - loss: 0.1026 - val_accuracy: 0.9652 - val_loss: 0.0697\n",
            "Epoch 7/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9657 - loss: 0.1034 - val_accuracy: 0.9733 - val_loss: 0.0768\n",
            "Epoch 8/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9659 - loss: 0.0911 - val_accuracy: 0.9786 - val_loss: 0.0660\n",
            "Epoch 9/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9814 - loss: 0.0836 - val_accuracy: 0.9599 - val_loss: 0.1255\n",
            "Epoch 10/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9676 - loss: 0.0987 - val_accuracy: 0.9786 - val_loss: 0.0620\n",
            "Epoch 11/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9765 - loss: 0.0672 - val_accuracy: 0.9840 - val_loss: 0.0501\n",
            "Epoch 12/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9829 - loss: 0.0642 - val_accuracy: 0.9786 - val_loss: 0.0543\n",
            "Epoch 13/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9865 - loss: 0.0415 - val_accuracy: 0.9786 - val_loss: 0.0735\n",
            "Epoch 14/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9846 - loss: 0.0458 - val_accuracy: 0.9866 - val_loss: 0.0564\n",
            "Epoch 15/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9917 - loss: 0.0274 - val_accuracy: 0.9813 - val_loss: 0.0770\n",
            "Epoch 16/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9898 - loss: 0.0283 - val_accuracy: 0.9786 - val_loss: 0.0732\n",
            "Epoch 17/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9912 - loss: 0.0378 - val_accuracy: 0.9840 - val_loss: 0.0510\n",
            "Epoch 18/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9906 - loss: 0.0326 - val_accuracy: 0.9759 - val_loss: 0.0559\n",
            "Epoch 19/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9900 - loss: 0.0322 - val_accuracy: 0.9759 - val_loss: 0.0587\n",
            "Epoch 20/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9882 - loss: 0.0338 - val_accuracy: 0.9813 - val_loss: 0.0666\n",
            "Epoch 21/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9975 - loss: 0.0196 - val_accuracy: 0.9866 - val_loss: 0.0449\n",
            "Epoch 22/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9988 - loss: 0.0109 - val_accuracy: 0.9866 - val_loss: 0.0522\n",
            "Epoch 23/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9927 - loss: 0.0208 - val_accuracy: 0.9840 - val_loss: 0.0620\n",
            "Epoch 24/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9937 - loss: 0.0223 - val_accuracy: 0.9920 - val_loss: 0.0514\n",
            "Epoch 25/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9998 - loss: 0.0075 - val_accuracy: 0.9840 - val_loss: 0.0499\n",
            "Epoch 26/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9943 - loss: 0.0176 - val_accuracy: 0.9840 - val_loss: 0.0877\n",
            "Epoch 27/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9920 - loss: 0.0195 - val_accuracy: 0.9866 - val_loss: 0.0461\n",
            "Epoch 28/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9920 - loss: 0.0206 - val_accuracy: 0.9759 - val_loss: 0.0627\n",
            "Epoch 29/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9986 - loss: 0.0125 - val_accuracy: 0.9893 - val_loss: 0.0344\n",
            "Epoch 30/30\n",
            "\u001b[1m47/47\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9930 - loss: 0.0140 - val_accuracy: 0.9893 - val_loss: 0.0446\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c0c04306380>"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Save F/M Model!\n",
        "\n",
        "- Now we can use the model in streamlit, gradio, or wherever."
      ],
      "metadata": {
        "id": "6llYsOzFnNco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# female_speaker_model.save('female_speaker_model.h5')\n",
        "# male_speaker_model.save('male_speaker_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bUPNPGhVMOr",
        "outputId": "642b7087-33ca-40c3-9bb1-c686acb175c4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}